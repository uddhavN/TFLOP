# Complete CPU-optimized TFLOP configuration with all required keys
# This config includes all the missing keys we've encountered during debugging

# Experiment metadata
exp_version: "cpu_v1.0"
exp_name: "tflop_cpu_experiment"
save_dir: "./results/cpu_experiments"
result_path: "./results/cpu_experiments"

# Required contrastive learning settings
use_RowWise_contLearning: false
use_ColWise_contLearning: true

contrastive:
  enabled: true
  loss_type: "ntxent"
  temp: 0.07
  proj_dim: 256

# Required special tokens and settings
special_chars: "<sos> <eos> <pad> <unk> <sep>"
use_OTSL: true
seed: 42

# Batch size settings (required by DataPLModule)
train_batch_size: 1
val_batch_size: 1
test_batch_size: 1

# Input configuration compatible with Swin Transformer
input_size:
  height: 224
  width: 224

max_target_length: 512
max_source_length: 512

# Model configuration with fixed Swin parameters
model:
  name: "donut"
  pretrained_path: "./pretrain_weights/donut-base-finetuned-cord-v2"
  
  # Encoder settings (these may be overridden by hardcoded values in TFLOP.py)
  encoder:
    image_size: [224, 224]
    patch_size: 4
    hidden_size: 768
    num_layers: 12
    num_heads: 12
    dropout: 0.1
    window_size: 7
    
  # Decoder settings
  decoder:
    vocab_size: 50265
    max_length: 512
    num_layers: 6
    hidden_size: 768
    num_heads: 12
    dropout: 0.1

# Training configuration optimized for CPU
training:
  max_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 1e-4
  warmup_steps: 50
  weight_decay: 0.01
  mixed_precision: false
  gradient_checkpointing: true
  gradient_clipping: 1.0
  val_check_interval: 0.5
  save_top_k: 1
  monitor: "val_loss"
  mode: "min"
  early_stopping:
    patience: 2
    monitor: "val_loss"
    mode: "min"

# Hardware configuration
hardware:
  devices: "cpu"
  accelerator: "cpu"
  precision: 32

# Logging configuration
logging:
  logger: "tensorboard"
  log_dir: "./logs/cpu_experiments"
  log_every_n_steps: 20

# Memory management
memory:
  empty_cache_frequency: 20
  gc_frequency: 100

# Dataset configuration
dataset_name: "pubtabnet_cpu"
dataset_path: "./TFLOP-dataset"

train:
  metadata_file: "meta_data/dataset_train_1percent.jsonl"
  images_dir: "images/train"
  pse_results_dir: "pse_results/train"
  image_size: [224, 224]
  max_sequence_length: 512
  cache_images: false
  lazy_loading: true

validation:
  metadata_file: "meta_data/validation.jsonl"
  images_dir: "images/validation"
  pse_results_dir: "pse_results/val"
  image_size: [224, 224]
  max_sequence_length: 512
  cache_images: false

# Dataloader configuration
dataloader:
  batch_size: 1
  num_workers: 2
  pin_memory: false
  persistent_workers: false
  prefetch_factor: 1
  drop_last: false
  shuffle_train: true
  shuffle_val: false

# Text processing configuration
text_processing:
  max_length: 512
  truncation: true
  padding: "max_length"

# Augmentation configuration (disabled for CPU efficiency)
augmentation:
  enabled: false
  rotation: false
  color_jitter: false

# Additional required fields
bbox_special_tokens: 225  # Adjusted for 224x224 input
version: "1.0"
framework: "pytorch_lightning"
task: "table_structure_recognition"

# Additional potential missing keys (add as needed)
window_size: 7
encoder_layer: [2, 2, 6, 2]
decoder_layer: 6
max_length: 512
name_or_path: "swin_base_patch4_window7_224"
max_position_embeddings: 1024
use_fast_decoder: true
bbox_token_cnt: 225
max_num_row: 50
max_num_col: 50
use_bbox_HiMulConET: false
use_imgRoiAlign: false
empty_cell_ptr_loss_coeff: 1.0
non_empty_cell_ptr_loss_coeff: 1.0